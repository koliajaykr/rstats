---
title: "üèãüèΩ‚Äç‚ôÄÔ∏èFitting Models<br>[Central Tendency]"
subtitle: "04 - Statistics with R"
author: "Ajay Kumar Koli"
format: 
  revealjs:
    incremental: true
    logo: "image/logo.png"
    footer: "[Nalanda Academy](<https://nalanda-academy.org/>)"
    scrollable: true
    slide-number: true
    transition: fade
    chalkboard: true
editor: visual
---

## Objectives: {background-image="image/building_model.png" background-opacity=".1"}

-   Statistical models

-   Central Tendency

-   The 'fit' of the mean: variance

-   Dispersion

## From the Dead

-   Zach and Milton agree to help each other.

-   But Zach suspects that Milton and Pincus are responsible behind the disappearance of Alice.

-   Zach agreed to learn stats from Milton

## *Gene Mixer*

![](image/gene_mixer.png)

# STATISTICAL MODEL

## Why do we need statistical models?...

-   Scientists try to make predictions about something in the real world ...

    -   it might be a psychological, societal, biological or economic process to which they do not have direct access.

-   Scientists cannot access the process directly ...

    -   they gather data and construct small-scale models of the process and use them to predict how these processes operate under more general conditions.

## Why do we need statistical models?...

-   We want our small-scale model to resemble the real situation as closely as possible so that the predictions we make about the real world are accurate.

-   The statistical model we build must represent the data collected (the *observed data*) as closely as possible.

-   The degree to which a statistical model represents the data collected is called the **fit** of the model.

------------------------------------------------------------------------

::: columns
::: {.column width="50%"}
-   Architect or engineer will build a model that resembles the real one as closely as possible.

-   Once the model is ready, it can be used to estimate what would happen to the real building.
:::

::: {.column width="50%"}
```{r}
#| out-width: "90%"
knitr::include_graphics("image/building_model.png")

```
:::
:::

## Sample Size: Bigger is Better

![](image/box_sample.png){fig-align="center"}

## The one and only statistical model...

$$outcome_i=(model)+error_i$$

-   "A statistical model is an equation that describes the phenomenon of interest"

-   Most statistical models are a variation on this one equation.

-   The model will change depending on what you are trying to achieve.

    -   Ultimately the word "model" is replaced with an equation that you believe summaries the pattern of data.

## The one and only statistical model...

$$
outcome_i=(model)+error_i
$$

$$
RAS_i=(model)+error_i
$$

## The one and only statistical model

-   Statistical models usually, but not always, contain variables and parameters.

    -   **Parameters** are (usually) constant values believed to represent some fundamental truth about the relations between variables in the models.

    -   Unlike variables, which are measured, parameters are estimated from the data.

## Simplest Possible Model

$$
outcome_i=(b_0)+error_i
$$

-   Confusius üßô‚Äç‚ôÇÔ∏è

    -   mean = $\bar{X}$

    -   correlation = $r$ (Greek symbol rho, $\rho$)

    -   linear model = $b$ (Greek symbol beta, $\beta$)

-   $b_0$ means we are predicting the outcome from zero other variables, that is, just from a single parameter.

------------------------------------------------------------------------

$$
outcome_i=(b_0+b_1X_i)+error_i
$$

-   A lot of time it is more interesting to see whether we can summarize an outcome variable by predicting from scores on another variable.

------------------------------------------------------------------------

$$
outcome_i=(b_0+b_1X_i)+error_i
$$

-   $i$ = particular entity

-   $outcome_i$ = outcome value for that particular entity

-   $X_i$ = score on the predictor variable

-   $b_1$ = predictor variable has a parameter attached to it which tells use something about the relationship between the predictor ($X_i$) and outcome

-   $b_0$ = is still there to tell us the overall levels of the outcome if the predictor variable was not in the model

## RAS Statistical Model

-   $outcome_i=(b_0+b_1X_i)+error_i$

-   $relationship\;satisfaction_i=(b_0+b_1length_i)+error_i$

## RAS Statistical Model

-   $outcome_i=(b_0+b_1X_{1i}+b_2X_{2i})+error_i$

-   $relationship\\satisfaction_i=(b_0+b_1length_i+b_2effort_i)+error_i$

-   "We use the sample data to estimate the value of the model parameters." $b$

-   "We use the sample data to *estimate (best guess)* what the population parameters are likely to be"

# üèãüèΩ‚Äç‚ôÄÔ∏èCENTRAL TENDENCY

## Alices' Relationship Assessment Scores

![](image/alice_ras.png){fig-align="center"}

## Alices' Relationship Assessment Scores

-   32, 30, 28, 30, 30, 29, 31, 29, 31, 11

-   A score that is very different from others is called an **outlier**.

-   Simplest model tries to summarize data in terms of a single parameter.

-   A popular choice would be a parameter that measures **central tendency**: a value that indicates the central point of a distribution of scores.

## The Mode

-   The score that occurs most frequently.

-   The tallest bar in the histogram and in the grouped frequency table, the score with the highest frequency.

## Downside of Mode

-   **Bimodal**: a distribution with two modes

-   **Multimodal**: Data sets with more than two modes

![](image/two_modes.png){fig-align="center"}

## ü§ØYour Turn

Calculate mode of Alice's RAS scores: 32, 30, 28, 30, 30, 29, 31, 29, 31, 11

## ü§ØYour Turn - Answer

![](image/ras_table.png){fig-align="center"}

## The Median

"the middle score when the scores are arranged in ascending order"

-   32, 30, 28, 30, 30, 29, 31, 29, 31, 11

-   position of the middle score$= \frac{(n+1)}{2}$

    -   $n$ = number of observations

## The Median

![](image/median_even.png){fig-align="center"}

## The Median

![](image/median_odd.png){fig-align="center"}

## The Mean

also called as arithmetic mean or average

::: columns
::: {.column width="50%"}
-   Population mean

-   $$
    \mu=\frac{\sum^n_{i=1}x_i}{N}
    $$
:::

::: {.column width="50%"}
-   Sample mean

-   $$
    \bar{X}(or M)=\frac{\sum^n_{i=1}x_i}{n}
    $$
:::
:::

------------------------------------------------------------------------

## üôÑ"A population of scores doesn't have to be scores from different entities, and neither does a sample."

## The Mean

-   $$\sum^n_{i=1}x_i=32+30+28+30+30+\\29+31+29+31+11=281$$

-   $$\bar{X}=\frac{\sum^n_{i=1}x_i}{n}=\frac{281}{10}=28.1$$

## Statistical Model

-   Alice's mean assessment of your relationship is 28.1

-   The fact that we are fitting a statistical model. ... We have reduced the data to a summary that does not perfectly represent the scores we observed.

## Statistical Model

::: columns
::: {.column width="50%"}
-   $$
    outcome_i=(b)+error_i
    $$

-   $$
    RAS_i=\bar{X}+error_i
    $$

-   $$
    RAS_i=28.1+error_i
    $$
:::

::: {.column width="50%"}
-   <div>

    ![](image/alice_ras.png){fig-align="center"}

    </div>
:::
:::

## ü§ØYour Turn

Measure mean after removing outlier value from the Alice's RAS scores: 32, 30, 28, 30, 29, 31, 29, 31, 11

## Your Turn Answer

Answer is 30, after removing score 11

![](image/mean_outlier.png){fig-align="center"}

## üò∫Message from Milton

![](image/mean_balancing.png){fig-align="center"}

## üò∫Message from Milton

![](image/mean_magic.png){fig-align="center" width="820"}

## The Mean

-   The mean divides the data in two, which makes it a reasonable summary of the data as a whole

-   The outlier has made the mean less representative of the data.

-   The mean defines it in terms of distance from the centre.

-   It uses every score in the data, and so it representative.

-   It tends to be quite stable across samples (less true of the median)

## The Median

-   The median measures it as the score at the centre, and

    -   It aims to split the data into two equal halves.

    -   It balance the data so that half of the scores are above it and half below.

## The Mode

-   The mode measures it as the most frequent score.

## üò∫Message from Milton

-   Which measure of central tendency to use, you need to think the type of data you have

    -   **Nominal variable** = Mode

    -   **Ordinal variable** = Median (because it based upon the scores being ordered)

        -   It is less influenced by extreme scores

    -   **Interval or Ratio** = Mean (because it is based on distances between scores and it assumes that the distance between scores is the same at all points along the scale).

## *But suddenly ...*

![](image/chippers.png){fig-align="center"}

# The Fit of the Mean: Variance {background-image="image/chippers.png" background-opacity="0.1" background-size="200%"}

## The Fit of the mean

We can assess how well the mean, or any other model, "fits" the data by looking at the errors in prediction."

$$
outcome_i=(model)+error_i\\error_i=outcome_i-model\\error_i=RAS-\mu
$$

## The fit of the mean

Alice's first week score is 32 but model value is 28.1. So, our model (mean in present case) underestimated the RAS score for week one.

$$
error_{week1}=outcome_{week1}-model\\error_{week1}=RAS_{week1}-\mu\\error_{week1}=32-28.1\\=3.9
$$

## Confusius üßô‚Äç‚ôÇÔ∏è

"Error in prediction from a model is sometimes known as **error**, and sometimes as **deviance** or **deviation** and other times as **residual**."

-   Deviance is the value of the outcome minus the value predicted from the model

-   $$
    error_i=outcome_i-model_i\\deviance_i=x_i=\mu
    $$

## How much error there is overall?

"if we want to know the total error or total deviance then we could add up the deviances for each data point"

-   $$
    total\;error=\sum^n_{i=1}(outcome_i-model_i)
    $$

-   $$
    total\;deviance=\sum^n_{i=1}(x_i-\mu)
    $$

## ü§ØYour Turn

Calculate error in the Alice's RAS score of each week, exclude 11

## Errors or Deviances

![](image/ras_error.png){fig-align="center" width="725"}

## Sum of Errors

$$
sum\;of\;errors=\sum^n_{i=1}(outcome_i-model_i)\\=(32-30)+(30-30)+(28-30)+(30-30)\\+(30-30)+(29-30)+(31-30)+(29-30)+\\(31-30)\\
=2+0+(-2)+0+0+(-1)+1+(-1)+1\\=2-2-1+1-1+1\\=0
$$

## Sum of Squared Errors

"we square the deviances ... we can add these squared deviances up to get the **sum of squared errors**, $SS$ (*sum of squares*)

## Sum of Squared Errors

$$
sum\;of\;squared\;errors(SS)=\sum^n_{i=1}(outcome_i-model_i)^2\\sum\;of\;squared\;errors(SS)=\sum^n_{i=1}(x_i-\mu)^2
$$

## *SS* of Alice's RAS score

![](image/ras_squared_errors.png){fig-align="center" width="900"}

## Sum of Squared Errors (*SS*)

"its size will depend on how many scores we have in the data. ... it is a nuisance if we want to compare the total error across samples of different sizes"

-   An easy solution is to divide by the number of scores (*N*).

## Variance

"when the model is mean, the mean squared error has a special name, the **variance**.

-   The symbol of variance in the population is $\sigma^2$.

## Variance

$$
mean\;squared\;error=\frac{SS}{N}=\frac{\sum^n_{i=1}(outcome_i-model_i)^2}{N}\\variance(\sigma^2)=\frac{SS}{N}=\frac{\sum^n_{i=1}(x_i-\mu)^2}{N}
$$

## Variance

$$
\sigma=variance=\frac{\sum^n_{i=1}(x_i-\mu)^2}{N}\\=\frac{12}{9}\\=1.33
$$

-   For Alice's RAS scores, we would say that the average error of the mean was 1.33 RAS units squared. ... convert it back to the original scale of measurement and take square root of the variance.

## Standard Deviation

$$
\sigma=\sqrt{variance}=\sqrt{\frac{\sum^n_{i=1}(x_i-\mu)^2}{N}}\\\sqrt{1.33}\\=1.15
$$

## Estimating the Fit of the Mean from a Sample

"if we use the equation that I just mentioned we will underestimate the variance and standard deviation of the population"

## Population

![](image/variance_overestimate.png){fig-align="center"}

## Biased Estimator

::: columns
::: {.column width="50%"}
A biased estimator is a statistic taken from a random sample that does not equal its corresponding population parameter.
:::

::: {.column width="50%"}
![](image/variance_estimate_sample.png){fig-align="center" width="374"}
:::
:::

## Unbiased Estimator

::: columns
::: {.column width="50%"}
It is a statistic from a sample that does equal the corresponding population parameter.
:::

::: {.column width="50%"}
![](image/variance_estimate_sample.png){fig-align="center" width="374"}
:::
:::

## ‚åöÔ∏è

-   sample variance is always biased in exactly the same way ... we can correct it to make it unbiased.

-   if we are using the sample to estimate the population variance then we divide by $N-1$, rather than by $N$.

## $N - 1$

$$
mean\;squared\;error=\frac{SS}{n-1}\\=\frac{\sum^n_{i=1}(outcome_i-model_i)^2}{n-1}
$$

## Variance

$$
variance(s^2)=\frac{SS}{n-1}=\frac{\sum^n_{i=1}(x_i-\bar{X}_i)^2}{n-1}\\=\frac{12}{8}\\=1.5
$$

-   Replace population mean $\mu$ with the sample mean $\bar{X}(M)$

-   Replace population variance $\sigma^2$ with sample variance $s^2$

-   We use $n$ instead of $N$.

## Standard Deviation

$$
s=\sqrt{variance}=\sqrt{\frac{\sum^n_{i=1}(x_i-\bar{X})^2}{n-1}}\\=\sqrt{1.5}\\=1.22
$$

## Why $N-1$?

![](image/degree-of-freedom.png){fig-align="center"}

## Why $N-1$?

::: columns
::: {.column width="50%"}
The point is that on average the estimate is better, not necessarily for a particular sample.
:::

::: {.column width="50%"}
![](image/degree-of-freedom.png){fig-align="center" width="367"}
:::
:::

## Degrees of Freedom

"are the number of scores that are free to vary"

-   $n-1$ is known as the **degrees of freedom** for the sample.

-   assuming that the sample and population means are the same, we fixing a parameter

-   if we hold one parameter constant then the degrees of freedom must be one less than the number of scores used to calculate the parameter.

## Degrees of Freedom

![](image/degree-of-freedom2.png){fig-align="center" width="679"}

## Outliers and Variance

-   The variance with the outlier is 37.43

-   The variance without the outlier is 1.5

-   Due to the outliers, variance is more biased than the mean.

# DISPERSION

## The Standard Deviation as an indicator of dispersion

-   Sum of square, variance and standard deviation, they all represent the same thing but

    -   One is a total

    -   One is an average, and

    -   One is the average converted back to the original units of measurement.

-   "Fit" as the dispersion or spread of data around the mean.

## The Standard Deviation as an indicator of dispersion

-   A small standard deviation (relative to the value of the mean) indicates that data points are close to the mean.

-   A large standard deviation (relative to the mean) indicates that the data points are distant from the mean.

-   A standard deviation of $0$ would mean that all of the scores were the same.

## Same *M* but different *SD*

![](image/dispersion_ras.png){fig-align="center" width="907"}

## Same *M* but different *SD*

-   If the mean "fits" the data well then most of the scores will cluster close to the mean and the resulting standard deviation will be small relative to the mean.

-   When the mean is a worse "fit" of the data, the scores will cluster more widely around the mean and the standard deviation will be larger.

## *SD* larger, Distribution fatter

![](image/sd-large-small.png){fig-align="center" width="907"}

## The Range and Interquartile Range

-   **Range:** Take the largest score and subtract it from the smallest score

-   But range value is influenced by the outliers.

-   **Interquartile Range**: Cut off the top and bottom 25% of scores and calculate the range of the middle 50% of scores.

    -   The interquartile range does not change if you exclude the extreme score.

## **Interquartile Range**

![](image/iqrange.png){fig-align="center"}

------------------------------------------------------------------------

### *Alice's average RAS score,* $\bar{X}=30$, $SD=1.22$, was higher than the national average

![](image/jigsaw-poster.png){fig-align="center" width="860"}
